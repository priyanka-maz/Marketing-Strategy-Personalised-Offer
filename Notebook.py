# -*- coding: utf-8 -*-
"""Marketing Strategy Personalised Offer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1T7w24NzIZpswSt74EtXrK5ZAE2dwbV9e
"""

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

import matplotlib.pyplot as plt
import seaborn as sns

import warnings
# Global rng instance
rng = np.random.RandomState(42)

# To show all output columns
pd.options.display.max_columns = None 

warnings.filterwarnings("ignore", category=UserWarning)

import os
for dirname, _, filenames in os.walk('Datasets'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

"""# Load Data

## Load raw data from CSV files
"""

data_train_raw = pd.read_csv('Datasets/train_data.csv')
data_test_raw  = pd.read_csv('Datasets/test_data.csv')

datasets = [data_train_raw.copy(), data_test_raw.copy()] # given pandas df's

target_id      = 'id'
target_feature = 'Offer Accepted'

"""## Show some basic info about given dataset"""

data_train_raw.info()

data_corr = datasets[0].corr()
plt.figure(figsize=(17, 20))
sns.heatmap(data_corr, annot=True, square=True, cmap="inferno")
plt.show()

#data_corr['Offer Accepted']*100

datasets[0].hist(bins=50, figsize=(20, 15))
plt.show()

data_train_raw.sample(n=10, random_state=42)

data_train_raw.isna().sum()

"""# Preprocessing

## Handling of Categorical Vairbales
"""

for dataset in datasets:
    dataset.loc[(dataset['travelled_more_than_25mins_for_offer'] == 1), 'travel_more'] = 3
    dataset.loc[((dataset['travelled_more_than_15mins_for_offer'] == 1) & (dataset['travelled_more_than_25mins_for_offer'] == 0) & (dataset['travelled_more_than_5mins_for_offer'] == 1)), 'travel_more'] = 2
    dataset.loc[(((dataset['travelled_more_than_5mins_for_offer'] == 1) & dataset['travelled_more_than_15mins_for_offer'] == 0) & (dataset['travelled_more_than_25mins_for_offer'] == 0)), 'travel_more'] = 1

"""## Dropping Unnecessary Columns"""

def dropcol(field):
    for dataset in datasets:
        dataset.drop([field], axis=1, inplace=True)

for i in ['travelled_more_than_25mins_for_offer', 'travelled_more_than_15mins_for_offer', 'travelled_more_than_5mins_for_offer', 
          'Prefer western over chinese', 'Cooks regularly', 'is foodie', 'visit restaurant with rating (avg)', 
          'Prefer home food', 'car', 'restuarant_same_direction_house', 'Restaur_spend_less_than20', 'has Children']: 
    dropcol(i)

"""## Label Encoder"""

'''
for dataset in datasets:
    dataset['offer expiration'] = dataset['offer expiration'].apply(lambda x: 0 if x == '10hours' else 1)
'''
datasets[0]['Offer Accepted'] = datasets[0]['Offer Accepted'].apply(lambda x: 0 if x == 'No' else 1)

from sklearn.preprocessing import LabelEncoder

def EncLabel(field):
    label = LabelEncoder()
    label.fit(datasets[0][field])
    
    for dataset in datasets:
        dataset[field] = label.transform(dataset[field])

#EncLabel('offer expiration')

"""## Ordinal Encoder"""

from sklearn.preprocessing import OrdinalEncoder

def EncOrdinal(field,values=None):
    if values:
        ordinal = OrdinalEncoder(handle_unknown="use_encoded_value", unknown_value=np.nan, categories=[values])
    else:
        ordinal = OrdinalEncoder(handle_unknown="use_encoded_value", unknown_value=np.nan)

    ordinal.fit(datasets[0][[field]])

    for dataset in datasets:
        dataset[field] = ordinal.transform(dataset[[field]])
        
EncOrdinal('income_range', ['Less than ₹12500', '₹12500 - ₹24999', '₹25000 - ₹37499', '₹37500 - ₹49999', '₹50000 - ₹62499', '₹62500 - ₹74999', '₹75000 - ₹87499', '₹87500 - ₹99999', '₹100000 or More'])
EncOrdinal('age', ['below21', '21', '26', '31', '36', '41', '46','50plus'])
EncOrdinal('Restaur_spend_greater_than20', ['never', 'less1', '1~3', '4~8', 'gt8'])
EncOrdinal('no_visited_Cold drinks', ['never', 'less1', '1~3', '4~8', 'gt8'])
EncOrdinal('no_visited_bars', ['never', 'less1', '1~3', '4~8', 'gt8'])
EncOrdinal('no_Take-aways', ['never', 'less1', '1~3', '4~8', 'gt8'])
EncOrdinal('Marital Status')
#EncOrdinal()
#EncOrdinal('Restaur_spend_less_than20', ['never', 'less1', '1~3', '4~8', 'gt8'])

"""## One Hot Encoding"""

def OneHot(field):
    dummies = pd.get_dummies(datasets[0][[field]])
    
    for dataset in datasets:
        dataset.drop(field, axis=1, inplace=True)
        dataset[dummies.columns] = dummies

#OneHot('Climate')

"""## Binning"""

'''
    # Continuous variables (binning)
    dataset['AgeBin'] = pd.cut(dataset['age'].astype(int), 5)
    dataset.drop(['age'], axis=1, inplace=True)
'''

"""## Binary Encoding"""

def BinEnc(field, df, values=None):
    vals = df[field].value_counts().keys()
    no_vals = len(vals)
    no_cols = no_vals.bit_length()
    if values:
        ordinal = OrdinalEncoder(handle_unknown="use_encoded_value", unknown_value=np.nan, categories=[values])
    else:
        ordinal = OrdinalEncoder(handle_unknown="use_encoded_value", unknown_value=np.nan)
    ordinal.fit(df[[field]])
    df[field] = ordinal.transform(df[[field]])
    bin_no = []
    d = {}
    for k in range(no_cols):
        f = field+"_"+str(k)
        d[f] = []

    for i in range(len(df[field])):
        j = df[field][i]
        bin_j = bin(int(j))[2:]
        diff = no_cols - len(bin_j)
        bin_j = "0" * diff + str(bin_j)
        bin_no.append(bin_j)

        for k in range(no_cols):
            f = field+"_"+str(k)
            d[f].append(int(bin_j[k]))

    for i in d.keys():
        df[i] = d[i]
    df.drop(field, inplace=True, axis=1)
    return df

for dataset in datasets:
    for field in ['drop location', 'Customer type', 'Climate', 'temperature', 'restaurant type',
            'offer expiration', 'gender', 'Job/Job Industry']:
        dataset = BinEnc(field, dataset)
    dataset = BinEnc('Qualification', dataset, ['Some High School', 'High School Graduate','Some college - no degree','Associates degree', 'Bachelors degree', 'Graduate degree (Masters or Doctorate)'])

"""## Drop some Binary Encoded columns"""

for i in ['Customer type_0','offer expiration_0', 'gender_0' , 'Job/Job Industry_1']:
    dropcol(i)

datasets[0].sample(10, random_state=42)

"""## Duplicate Row Elimination"""

print('Duplicate rows:', datasets[0].duplicated().value_counts())
datasets[0].drop_duplicates(inplace=True)

"""## Split into features and label"""

X_train = datasets[0].drop('Offer Accepted', axis=1, inplace=False)
y_train = datasets[0][['Offer Accepted']]
features = X_train.columns
X_test = datasets[1]

X_train.isna().sum()

"""## Impute NaN values

### KNN Imputer
"""

'''
from sklearn.impute import KNNImputer
imputer = KNNImputer()

X_train = pd.DataFrame(imputer.fit_transform(X_train), index=X_train.index, columns=X_train.columns)
X_test  = pd.DataFrame(imputer.transform(X_test), index=X_test.index, columns=X_test.columns)
'''

"""### Simple Imputer"""

from sklearn.impute import SimpleImputer
imputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')

X_train = pd.DataFrame(imputer.fit_transform(X_train), index=X_train.index, columns=X_train.columns)
X_test  = pd.DataFrame(imputer.transform(X_test), index=X_test.index, columns=X_test.columns)

"""## Outlier Elimination"""

from sklearn.neighbors import LocalOutlierFactor

df = pd.concat([X_train, y_train], axis=1)

outlier = LocalOutlierFactor(algorithm='brute', contamination=1e-3, n_jobs=-1)
outliers = outlier.fit_predict(df)
outliers = (outliers < 0).tolist()
print('Outliers: ', np.unique(outliers, return_counts=True))
df.drop([i for i in range(len(outliers)) if outliers[i]], axis=0, inplace=True)

print(X_train.shape)
X_train = df.drop([target_feature], axis=1, inplace=False)
y_train = df[target_feature]
print(X_train.shape)

"""## Scalers : Standard and Min-Max Scaler"""

'''
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train = pd.DataFrame(scaler.fit_transform(X_train), index=X_train.index, columns=X_train.columns)
X_test = pd.DataFrame(scaler.fit_transform(X_test), index=X_test.index, columns=X_test.columns)
'''

from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
X_train = pd.DataFrame(scaler.fit_transform(X_train), index=X_train.index, columns=X_train.columns)
X_test = pd.DataFrame(scaler.fit_transform(X_test), index=X_test.index, columns=X_test.columns)

"""## Polynomial Transform"""

'''
from sklearn.preprocessing import PolynomialFeatures

poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=True)
X_train = poly.fit_transform(X_train)
X_test = poly.transform(X_test)
X_train.shape
'''

"""## Balancer"""

'''
from imblearn.over_sampling import RandomOverSampler

over_sampler = RandomOverSampler(random_state=rng)
X_train, y_train = over_sampler.fit_resample(X_train, y_train)
print('After balancing: ', X_train.shape)
'''

"""# Models"""

from sklearn import model_selection
cv_split = model_selection.ShuffleSplit(n_splits=10, test_size=0.30, train_size=0.60, random_state=rng)

from xgboost import XGBClassifier

from sklearn import dummy
from sklearn import discriminant_analysis
from sklearn import ensemble
from sklearn import gaussian_process
from sklearn import linear_model
from sklearn import naive_bayes
from sklearn import neighbors
from sklearn import neural_network
from sklearn import svm
from sklearn import tree

'''
models = [
    dummy.DummyClassifier(strategy='most_frequent'),

    discriminant_analysis.LinearDiscriminantAnalysis(),
    #discriminant_analysis.QuadraticDiscriminantAnalysis(),

    #ensemble.AdaBoostClassifier(),
    ensemble.BaggingClassifier(),
    #ensemble.ExtraTreesClassifier(),
    ensemble.GradientBoostingClassifier(),
    ensemble.RandomForestClassifier(),

    #gaussian_process.GaussianProcessClassifier(),
    
    linear_model.LogisticRegressionCV(),
    #linear_model.PassiveAggressiveClassifier(),
    linear_model.Perceptron(),
    #linear_model.RidgeClassifierCV(),
    linear_model.SGDClassifier(),

    #naive_bayes.BernoulliNB(),
    #naive_bayes.GaussianNB(),

    neighbors.KNeighborsClassifier(),
    
    neural_network.MLPClassifier(),

    #svm.LinearSVC(),
    #svm.NuSVC(probability=True),
    #svm.SVC(probability=True),

    tree.DecisionTreeClassifier(),
    #tree.ExtraTreeClassifier(),

    XGBClassifier()
]

# Model Metrics table
metrics = pd.DataFrame(columns=['Name', 'Parameters', 'Test_Score', 'Test_Score_3SD', 'Fit_Time'])
model_predictions = pd.DataFrame(columns=[model.__class__.__name__ for model in models])

for i in range(len(models)):
    cv_results = model_selection.cross_validate(models[i], X_train, y_train, cv=cv_split)

    metrics.loc[i, 'Name']           = models[i].__class__.__name__
    metrics.loc[i, 'Parameters']     = str(models[i].get_params())
    metrics.loc[i, 'Test_Score']     = cv_results['test_score'].mean()   
    metrics.loc[i, 'Test_Score_3SD'] = cv_results['test_score'].mean() - 3 * cv_results['test_score'].std()
    metrics.loc[i, 'Fit_Time']       = cv_results['fit_time'].mean()
    
    models[i].fit(X_train, y_train)
    model_predictions[models[i].__class__.__name__] = models[i].predict(X_train)
'''

'''
sort_metric = 'Test_Score_3SD'

metrics.sort_values(by=[sort_metric], ascending=False, inplace=True)
metrics
'''

"""## Correlation b/w models"""

'''
plt.figure(figsize=(15, 12))
sns.heatmap(model_predictions.corr(), cmap="inferno", square=True, annot=True)
plt.show()
'''

"""## Baseline"""

from sklearn.model_selection import GridSearchCV

from sklearn.dummy import DummyClassifier

clf_dum = DummyClassifier(strategy='most_frequent')

"""## Support Vector Machine"""

estimator = svm.SVC(random_state=rng)
param_grid = {'C': [0.0001, 1, 0.5],
              'kernel': ['linear', 'rbf']}
'''
clf = GridSearchCV(estimator, param_grid, n_jobs=-1, cv=cv_split, scoring='f1_macro')
clf.fit(X_train, y_train)
clf.best_params_
'''
clf_svc = svm.SVC(C=0.0001, kernel='rbf', random_state=rng)

"""## K Nearest Neighbors"""

estimator = neighbors.KNeighborsClassifier()
param_grid = {'n_neighbors': [3,5],
              'weights': ['uniform', 'distance'],
              'algorithm': ['ball_tree','brute','auto']}
'''
clf = GridSearchCV(estimator, param_grid, n_jobs=-1, cv=cv_split, scoring='f1_macro')
clf.fit(X_train, y_train)
clf.best_params_
'''
clf_knn = neighbors.KNeighborsClassifier(algorithm='brute', n_neighbors=5, weights='uniform')

"""## Neural Network: Multi-Layer Perceptron """

estimator = neural_network.MLPClassifier(random_state=rng)
param_grid = {'activation': ['logistic', 'relu'],
              'solver': ['adam', 'lbfgs'],
              'max_iter': [600, 700]}
'''
clf = GridSearchCV(estimator, param_grid, n_jobs=-1, cv=cv_split, scoring='f1_macro')
clf.fit(X_train, y_train)
clf.best_params_
'''
clf_mlp = neural_network.MLPClassifier(solver='adam', activation='logistic', max_iter=700, random_state=rng)

"""## Stochastic Gradient Descent"""

estimator = linear_model.SGDClassifier(random_state=rng)
param_grid = {'loss': ['modified_huber','perceptron'],
              'eta0': [0.01, 0.02],
              'penalty': ['l1', 'l2'],
              'learning_rate': ['optimal', 'adaptive']}

'''
clf = GridSearchCV(estimator, param_grid, n_jobs=-1, cv=cv_split, scoring='f1_macro')
clf.fit(X_train, y_train)
clf.best_params_
'''

clf_sgd = linear_model.SGDClassifier(eta0=0.01, learning_rate='adaptive', loss='modified_huber', penalty='l2', random_state=rng)

"""## XGB Classifier"""

estimator = XGBClassifier(random_state=rng)
param_grid = {
    'n_estimators': [100, 200, 500, 1000],
    'max_depth': [1,2,3,4,5,6],
    'learning_rate':[0.001,0.01,0.1,1,10],
    'subsample':[0.3,0.5,0.7],
    'colsample_bytree':[0.3,0.5,0.7],
    'reg_alpha ':[0.05,0.1,0.3,1,5],
    'gamma':[0,1,5]
}

'''
clf = GridSearchCV(estimator, param_grid, n_jobs=-1, cv=3, scoring='f1_macro')
clf.fit(X_train, y_train)
print(clf.best_params_)
'''

xgboost = XGBClassifier(subsample=0.7,reg_alpha=0.05,n_estimators=200,max_depth=5,learning_rate=0.1,gamma=0,colsample_bytree=0.7, nthread=-1,random_state=rng)

"""## Gradient Boosting"""

estimator = ensemble.GradientBoostingClassifier(random_state=rng)
param_grid = {'n_estimators': [100, 200, 300],
              'min_samples_leaf': [1, 2, 3, 4]}

'''
clf = GridSearchCV(estimator, param_grid, n_jobs=-1, cv=cv_split, scoring='f1_macro')
clf.fit(X_train, y_train)
print(clf.best_params_)
'''

clf_gb = ensemble.GradientBoostingClassifier(min_samples_leaf=4, n_estimators=200, random_state=rng)

"""## Random Forest """

from sklearn.model_selection import RandomizedSearchCV

estimator = ensemble.RandomForestClassifier(random_state=rng)
params_grid = {
    'n_estimators':[100,200,300],
    'criterion':['gini','entropy'],
    'min_samples_split':[5,10,25],
    'max_samples':[0.3,0.5,0.7], 
    'min_samples_leaf': [1, 2, 3, 4]
}

'''
clf = GridSearchCV(estimator, param_grid, n_jobs=-1, cv=cv_split, scoring='f1_macro')
clf.fit(X_train, y_train)
print(clf.best_params_)
'''

clf_rf = ensemble.RandomForestClassifier(n_estimators=300, min_samples_split=10, max_samples=0.5, criterion='entropy', random_state=rng, n_jobs=-1)

"""## Bagging"""

estimator = ensemble.BaggingClassifier(random_state=rng)
param_grid = {'base_estimator': [clf_svc, clf_rf],
              'n_estimators': [10, 20],
              'max_features': [0.05, 0.5, 1]}
'''
clf = GridSearchCV(estimator, param_grid, n_jobs=-1, cv=cv_split, scoring='f1_macro')
clf.fit(X_train, y_train)
clf.best_params_
'''
clf_bag = ensemble.BaggingClassifier(base_estimator=clf_rf, n_estimators=20, max_features=0.5, random_state=rng)

"""## Perceptron"""

estimator = linear_model.Perceptron(random_state=rng)
param_grid = {'penalty':['l2','l1','elasticnet']}

'''
clf = GridSearchCV(estimator, param_grid, n_jobs=-1, cv=cv_split, scoring='f1_macro')
clf.fit(X_train, y_train)
clf.best_params_
'''

clf_pc = linear_model.Perceptron(penalty='l1', random_state=rng)

"""## Voting Classifier"""

from sklearn.ensemble import VotingClassifier

model_vote = VotingClassifier(voting='hard', estimators=[
    ('rfc', clf_rf),
    ('xgb', xgboost)
])

"""## Feature Selection

### Recursive Feature Elimination
"""

'''
from sklearn import feature_selection

features = X_train.columns

rfe = feature_selection.RFECV(clf_rf, scoring='f1_macro', cv=5)
rfe.fit(X_train, y_train)
rfe_cols = X_train.columns.values[rfe.get_support()]

print('Remaining features: ', rfe_cols)

rfe_results = model_selection.cross_validate(clf_rf, X_train, y_train, cv=5)
print('Test_Score: '  , rfe_results['test_score'].mean())   
print('Test_Score_3SD', rfe_results['test_score'].mean() - 3 * rfe_results['test_score'].std())

X_train = X_train[rfe_cols]
X_test  = X_test[rfe_cols]
'''

"""### K Best Features"""

'''
from sklearn.feature_selection import SelectKBest, f_classif

selector = SelectKBest(f_classif, k=28)
selector.fit(X_train, y_train)

selected_features = features[selector.get_support()]
print('Selected features: ', selected_features)

X_train = X_train[selected_features]
X_test  = X_test[selected_features]
'''

"""## Fit Chosen Model with Training Data"""

#clf = model_vote
clf = clf_rf
clf.fit(X_train, y_train)

"""## Predict Test Label with Chosen Model"""

y_pred = clf.predict(X_train)
data_test_raw['Offer Accepted'] = clf.predict(X_test)
data_test_raw['Offer Accepted'] = data_test_raw['Offer Accepted'].apply(lambda x: 'Yes' if x == 1 else 'No')

"""## Visualize using a Confusion Matrix"""

from sklearn.metrics import ConfusionMatrixDisplay

ConfusionMatrixDisplay.from_predictions(y_train, clf.predict(X_train), cmap="inferno")
plt.show()

"""## Classification Report"""

from sklearn.metrics import classification_report
print(classification_report(y_train, clf.predict(X_train)))

data_test_raw['id'] = list(data_test_raw.index)

data_test_raw[['id','Offer Accepted']].to_csv('Datasets/submission.csv', index=None)